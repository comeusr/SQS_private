AdamW optimizer
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Model Device:  cuda:0
  0%|                                                                                                                       | 0/4210 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/wang4538/DGMS-master/scripts/../glue_training.py", line 519, in <module>
    main(args)
  File "/home/wang4538/DGMS-master/scripts/../glue_training.py", line 395, in main
    optimizer.step()
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/accelerate/optimizer.py", line 165, in step
    self.scaler.step(self.optimizer, closure)
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 451, in step
    self.unscale_(optimizer)
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
                                              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
