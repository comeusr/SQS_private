Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
AdamW optimizer
Model Device:  cuda:0
  0%|                                                                                                                       | 0/4210 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/wang4538/DGMS-master/scripts/../glue_training.py", line 563, in <module>
    main(args)
  File "/home/wang4538/DGMS-master/scripts/../glue_training.py", line 439, in main
    optimizer.step()
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 75, in wrapper
    return wrapped(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/accelerate/optimizer.py", line 165, in step
    self.scaler.step(self.optimizer, closure)
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 447, in step
    self.unscale_(optimizer)
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 337, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
                                              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 259, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
