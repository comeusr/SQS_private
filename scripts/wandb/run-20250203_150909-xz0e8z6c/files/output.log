AdamW optimizer
Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Model Device:  cuda:0
/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.
  warnings.warn(
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0] Graph break from `Tensor.item()`, consider setting:
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0]     torch._dynamo.config.capture_scalar_outputs = True
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0] or:
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0]     env TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0] to include these operations in the captured graph.
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0]
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0] Graph break: from user code at:
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0]   File "/home/wang4538/miniconda3/lib/python3.12/site-packages/transformers/modeling_flash_attention_utils.py", line 54, in torch_dynamo_resume_in__get_unpad_data_at_53
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0]     max_seqlen_in_batch = seqlens_in_batch.max().item()
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0]
W0203 15:09:21.384000 19079 site-packages/torch/_dynamo/variables/tensor.py:776] [12/0]
W0203 15:09:26.082000 19079 site-packages/torch/_dynamo/convert_frame.py:844] [7/8] torch._dynamo hit config.cache_size_limit (8)
W0203 15:09:26.082000 19079 site-packages/torch/_dynamo/convert_frame.py:844] [7/8]    function: 'forward' (/home/wang4538/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py:149)
W0203 15:09:26.082000 19079 site-packages/torch/_dynamo/convert_frame.py:844] [7/8]    last reason: 7/0: L['self'].layer_idx == 0
W0203 15:09:26.082000 19079 site-packages/torch/_dynamo/convert_frame.py:844] [7/8] To log all recompilation reasons, use TORCH_LOGS="recompiles".
W0203 15:09:26.082000 19079 site-packages/torch/_dynamo/convert_frame.py:844] [7/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.
  0%|                                                                                                                       | 0/4210 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/wang4538/DGMS-master/scripts/../glue_training.py", line 533, in <module>
    main(args)
  File "/home/wang4538/DGMS-master/scripts/../glue_training.py", line 409, in main
    optimizer.step()
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/accelerate/optimizer.py", line 165, in step
    self.scaler.step(self.optimizer, closure)
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 451, in step
    self.unscale_(optimizer)
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
                                              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/wang4538/miniconda3/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
