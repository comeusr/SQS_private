Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
AdamW optimizer
Model Device:  cuda:0
  0%|                                                                                                                       | 0/8420 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/wang4538/DGMS-master/scripts/../glue_training.py", line 574, in <module>
    main(args)
  File "/home/wang4538/DGMS-master/scripts/../glue_training.py", line 449, in main
    accelerator.clip_grad_norm_(model.parameters(), max_norm=1.0)
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/accelerate/accelerator.py", line 2396, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/accelerate/accelerator.py", line 2340, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 337, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
                                              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/wang4538/miniconda3/envs/LLM/lib/python3.12/site-packages/torch/amp/grad_scaler.py", line 259, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
